# -*- coding: utf-8 -*-
# """Copy of time_series.ipynb

# Automatically generated by Colaboratory.

# Original file is located at
#     https://colab.research.google.com/drive/14CNI4fWs4PjsNBNKQzsXKhgjMbvGVi_J
# """

# !pip install -U -q PyDrive
# from pydrive.auth import GoogleAuth
# from pydrive.drive import GoogleDrive
# from google.colab import auth
# from oauth2client.client import GoogleCredentials
# # Authenticate and create the PyDrive client.
# auth.authenticate_user()
# gauth = GoogleAuth()

# gauth.credentials = GoogleCredentials.get_application_default()
# drive = GoogleDrive(gauth)

# # link='https://drive.google.com/open?id=1CAD4CCx4Tgx0cU-zlt-EdlS0EjE73Fvg'
# link='https://drive.google.com/open?id=1CAD4CCx4Tgx0cU-zlt-EdlS0EjE73Fvg'
# fluff, id = link.split('=')
# print (id)
# downloaded = drive.CreateFile({'id':id}) 
# downloaded.GetContentFile('T3_merge_data.csv')
# from __future__ import absolute_import, division, print_function, unicode_literals
# try:
# #   %tensorflow_version only exists in Colab.
#     tensorflow_version 2.0
# except Exception:
#     pass
import streamlit as st
st.title ('Agitator Amps a forecasting and feature effect')

st.write('Importing the required python libraries')

import tensorflow as tf
#import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
#import os
import pandas as pd
import streamlit as st
import pandas as pd
#from enum import Enum
#from io import BytesIO, StringIO
#from typing import Union
def data_upload():
    global df1
    uploaded_file=st.sidebar.file_uploader(label='upload your csv or excel file.' ,type=['csv','xlsx'])
    if uploaded_file is not None:
        st.write('Reading the uploaded file')
        df1=pd.read_csv(uploaded_file)
    df=df1.copy()
    st.write('printing first five rows of the data')
    st.dataframe(df.head())
    return df 

    
#df=df1.copy()
#st.write('printing first five rows of the data')
#st.dataframe(df.head())
# df2=df3[(df3['Timestamp']>'06-02-2019 00:01:00') & (df3['Timestamp']<'06-02-2019 23:59:00')]
def columns():
    col=['Timestamp','TR3 V313 AGITATOR AMPS','T3 ES RSD V313 S','TR3 RSC#A INLET TEMP','TR3 RSC#B INLET TEMP','TR3 RSC#C INLET TEMP']
    return col
# 'TR3 RSC#A DILUENT FLOW',
# 'TR3 RSC#B DILUENT FLOW',
# 'TR3 RSC#C DILUENT FLOW',
# 'TR3 RSC#D DILUENT FLOW',
# 'TR3 RSC#E DILUENT FLOW',
# 'TR3 P307 OUTLET FLOW',
# 'TR3 RSC#A INLET FLOW CONTR',
# 'TR3 RSC#B INLET FLOW CONTR',
# 'TR3 RSC#C INLET FLOW CONTR',
# 'TR3 RSC#D INLET FLOW CONTR',
# 'TR3 RSC#E INLET FLOW CONTR',
# # 'TR3 RSC#F INLET FLOW CONTR',
# 'TR3 P301A/B S/B FLOW CONTR',
# 'TR3 ES REJ-RSCF A-C FLOW CONTR',
# 'TR3 ES REJ-RSCF D-F FLOW CONTR',
# 'TR3 P316 OUTLET FLOW CONTR',
# 'TR3 P307A/B S/B FLOW CONTR',
# 'TR3 P306A AMPS',
# 'TR3 P306B AMPS',
# 'TR3 P307A AMPS',
# 'TR3 P307B AMPS',
# 'TR3 RSC#A AMPS',
# 'TR3 RSC#B AMPS',
# 'TR3 RSC#C AMPS',
# 'TR3 RSC#D AMPS',
# 'TR3 RSC#E AMPS',
# 'TR3 RSC#F AMPS',
# 'TR3 V322 LEVEL', 
# 'TR3 V322 LEVEL',  
# 'TR3 V300321 LEVEL',  
# 'TR3 V300321 LEVEL',  
# 'TR3 V313 LEVEL',  
# 'TR3 V313 LEVEL',  
# 'TR3 V313 LEVEL',  
# 'TR3 V314 LEVEL',  
# 'TR3 V314 LEVEL',  
# 'TR3 V314 LEVEL',  
# 'TR3 V307 LEVEL',  
# 'TR3 V307 LEVEL',
# 'TR3 V309 LEVEL',  
# 'TR3 V309 LEVEL',  
# 'TR3 RSC#A INLET SOLIDS',
# 'TR3 RSC#B INLET SOLIDS',
# 'TR3 RSC#C INLET SOLIDS',
# 'TR3 RSC#D INLET SOLIDS',
# 'TR3 RSC#E INLET SOLIDS',
# 'TR3 RSC#F FEED SOLIDS',
# 'TR3 RS RFD OVR FLW I/L TEMP',
# 'TR3 ESRSD V313 TEMP',
# # 'TR3 P306A/B O/L TEMP',
# 'TR3 ESRSD V314 TEMP',
# 'TR3 ESRSD V314 TEMP',
# 'TR3 ESRFD TEMP',
# 'TR3 PS RFD TEMP',
# 'TR3 RSC#A INLET TEMP',
# 'TR3 RSC#B INLET TEMP',
# 'TR3 RSC#C INLET TEMP']
def copy1():
    df=data_upload()
    col=columns()
    df=df[col]
    st.write('Printing the feature names for model building')
    st.write(df.columns)
    st.write('Data preprocessing')
    features = df
    features.index = df['Timestamp']
    features.drop(columns=['Timestamp'],inplace=True)
    return features
    
    
    
from sklearn import pipeline
from sklearn.feature_selection import VarianceThreshold
from sklearn.preprocessing import StandardScaler, MinMaxScaler
def sacling():
    features=copy1()
    scaler=pipeline.Pipeline(steps=[
      ('z-scale', StandardScaler()),
         ('minmax', MinMaxScaler(feature_range=(-1, 1))),
         ('remove_constant', VarianceThreshold())])
    df=scaler.fit_transform(features)
    return df

# Commented out IPython magic to ensure Python compatibility.

mpl.rcParams['figure.figsize'] = (8, 6)
mpl.rcParams['axes.grid'] = False

def create_time_steps(length):
    return list(range(-length, 0))

def show_plot(plot_data, delta, title):
    labels = ['History', 'True Future', 'Model Prediction']
    marker = ['.-', 'rx', 'go']
    time_steps = create_time_steps(plot_data[0].shape[0])
    if delta:
        future = delta
    else:
        future = 0

    plt.title(title)
    for i, x in enumerate(plot_data):
        if i:
            plt.plot(future, plot_data[i], marker[i], markersize=10,
               label=labels[i])
        else:
            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])
        plt.legend()
        plt.xlim([time_steps[0], (future+5)*2])
        plt.xlabel('Time-Step')
        return plt

def multivariate_data(dataset, target, start_index, end_index, history_size,
                      target_size, step, single_step=False):
    data = []
    labels = []

    start_index = start_index + history_size
    if end_index is None:
        end_index = len(dataset) - target_size

    for i in range(start_index, end_index):
        indices = range(i-history_size, i, step)
        data.append(dataset[indices])

        if single_step:
            labels.append(target[i+target_size])
        else:
            labels.append(target[i:i+target_size])

    return np.array(data), np.array(labels)

# def multivariate_data(dataset, target,start_index, end_index, history_size,
#                       target_size, step, single_step=False):
#   data = []
#   labels = []
#   # labels1=[]
#   # labels2=[]
  
#   start_index = start_index + history_size
#   if end_index is None:
#     end_index = len(dataset) - target_size
#   # a=0
#   for i in range(start_index, end_index,history_size):
#     # i=a+i
#     indices = range(i-history_size, i, step)
#     data.append(dataset[indices])

#     if single_step:
#       labels.append(target[i+target_size])
#     else:
#       labels.append(target[i:i+target_size])

#     # if single_step:
#     #   labels1.append(target1[i+target_size])
#     # else:
#     #   labels1.append(target1[i:i+target_size])
#     # if single_step:
#     #   labels2.append(target2[i+target_size])
#     # else:
#     #   labels2.append(target2[i:i+target_size])
#     # a=a+15  

#   return np.array(data), np.array(labels)
def model_dev():
    df=sacling()
    future_target = 10
    past_history=60
    TRAIN_SPLIT=100000
    STEP=1
    x_train_multi, y_train_multi = multivariate_data(df, df[:, 0], 0,
                                                     TRAIN_SPLIT, past_history,
                                                     future_target, STEP)
    x_val_multi, y_val_multi = multivariate_data(df, df[:, 0],
                                                 TRAIN_SPLIT,150000, past_history,
                                                 future_target, STEP)

    x_val_multi, y_val_multi = multivariate_data(df, df[:, 0],
                                                 0,1438, past_history,
                                                 future_target, STEP)
    BATCH_SIZE=200
    BUFFER_SIZE=50
    val_data_multi = tf.data.Dataset.from_tensor_slices((x_val_multi, y_val_multi))
    val_data_multi = val_data_multi.batch(BATCH_SIZE).repeat()

    print ('Single window of past history : {}'.format(x_train_multi[0].shape))
    print ('\n Target solids to predict : {}'.format(y_train_multi[0].shape))

    BATCH_SIZE=200
    BUFFER_SIZE=50
    # train_data_multi = tf.data.Dataset.from_tensor_slices((x_train_multi, y_train_multi))
    # train_data_multi = train_data_multi.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()

    # val_data_multi = tf.data.Dataset.from_tensor_slices((x_val_multi, y_val_multi))
    # val_data_multi = val_data_multi.batch(BATCH_SIZE).repeat()
    st.write('Building the neural network flow')

    multi_step_model = tf.keras.models.Sequential()
    multi_step_model.add(tf.keras.layers.LSTM(5,
                                              return_sequences=True,
                                              input_shape=x_val_multi.shape[-2:],activation='relu', recurrent_activation='sigmoid'))
    # multi_step_model.add(tf.keras.layers.BatchNormalization())
    # multi_step_model.add(tf.keras.layers.Dropout(0.1))
    # multi_step_model.add(tf.keras.layers.LSTM(5,return_sequences=True,
    #                                           activation='tanh'
    #                                           ))
    # multi_step_model.add(tf.keras.layers.BatchNormalization())
    # multi_step_model.add(tf.keras.layers.Dropout(0.3))
    # multi_step_model.add(tf.keras.layers.LSTM(5,return_sequences=True,
    #                                           activation='tanh'
    #                                           ))
    # multi_step_model.add(tf.keras.layers.BatchNormalization())
    # multi_step_model.add(tf.keras.layers.Dropout(0.3))
    # multi_step_model.add(tf.keras.layers.LSTM(5,return_sequences=True,activation='tanh'))
    # multi_step_model.add(tf.keras.layers.LayerNormalization(axis=1 , center=True , scale=True))
    # multi_step_model.add(tf.keras.layers.LSTM(5,return_sequences=True,activation='tanh'))
    # multi_step_model.add(tf.keras.layers.BatchNormalization())
    multi_step_model.add(tf.keras.layers.LSTM(5, activation='relu'))
    multi_step_model.add(tf.keras.layers.Dense(10,activation='linear', kernel_regularizer=tf.keras.regularizers.l1(0.01)))
    adam=tf.keras.optimizers.Adam(learning_rate=0.04, beta_1=0.9, beta_2=0.99, epsilon=1e-03)
    multi_step_model.compile(optimizer='adam', loss='mae',metrics=['mae'])
    return multi_step_model
# multi_step_model.compile(optimizer=tf.keras.optimizers.RMSprop(clipvalue=1.0), loss='mae')

# import tensorflow_addons as tfa

# for e in zip(multi_step_model.layers[0].trainable_weights, multi_step_model.layers[0].get_weights()):
#     print('Param %s:\n%s' % (e[0],e[1]))

# """Plotting a sample data-point."""

def multi_step_plot(history, true_future, prediction):
    plt.figure(figsize=(12, 6))
    num_in = create_time_steps(len(history))
    num_out = len(true_future)

    plt.plot(num_in, np.array(history[:, 1]), label='History')
    plt.plot(np.arange(num_out)/STEP, np.array(true_future), 'bo',
           label='True Future')
    if prediction.any():
        plt.plot(np.arange(num_out)/STEP, np.array(prediction), 'ro',
             label='Predicted Future')
    plt.legend(loc='upper left')
    plt.show()

# from google.colab import drive
# drive.mount('/content/gdrive')

# """In this plot and subsequent similar plots, the history and the future data are sampled every hour."""

# checkpoint_path =  "/content/gdrive/My Drive/savemodel/15mymode.h5"
# checkpoint_dir = os.path.dirname(checkpoint_path)

# # batch_size = 32

# # # Create a callback that saves the model's weights every 5 epochs
# cp_callback = tf.keras.callbacks.ModelCheckpoint(
#     filepath=checkpoint_path, 
#     verbose=1, 
#     save_weights_only=True,
#     save_best_only=True)

# # # Save the weights using the `checkpoint_path` format
# multi_step_model.save_weights(checkpoint_path.format(epoch=0))
# EPOCHS=50
# EVALUATION_INTERVAL=100
# multi_step_1 = multi_step_model.fit(train_data_multi, epochs=EPOCHS,
#                                           steps_per_epoch=EVALUATION_INTERVAL,
#                                           validation_data=val_data_multi, validation_steps=50
#                                            ,callbacks=cp_callback,verbose=1)
def weights():
    st.write('Weight updation through BP algorithm')
    multi_step_model=model_dev()
    multi_step_model.load_weights("C:/Users/Vilas/Desktop/RELIANCE/RIL data/PX4 crystallizer/15mymode.h5")
    return multi_step_model

# pyplot.plot(multi_step_model.history.history['loss'])
# pyplot.plot(multi_step_model.history.history['val_loss'])
# pyplot.title('model train vs validation loss')
# pyplot.ylabel('loss')
# pyplot.xlabel('epoch')
# pyplot.legend(['train', 'validation'], loc='upper right')
# pyplot.show()

# import matplotlib.pyplot as pyplot

#import numpy as np
# from sklearn.externals import joblib
# pip install shap
# import shap
#from matplotlib import pyplot as plt
def model_predictons():
    k=0
    for j in range(1,10):
        k=k+1
        st.set_option('deprecation.showPyplotGlobalUse', False)
        st.write('forecasting the Amps values')
        multi_step_plot=weights()
        st.pyplot(multi_step_plot(x_val_multi[j],y_val_multi[j],multi_step_model.predict(x_val_multi)[j]))
        orig_out=multi_step_model.predict(x_val_multi)[j]
        st.write('Calculating the effect of each feature on model forecasting')
        for i in range(5):  # iterate over the three features
            new_x = x_val_multi[j:k+1].copy()
            perturbation = np.random.normal(0.0, 1, size=new_x.shape[:2 ])
            # print(perturbation[:, :i])
            new_x[:, :, i] = new_x[:, :, i] + perturbation
            perturbed_out = multi_step_model.predict(new_x)
            effect = ((orig_out - perturbed_out) ** 2).mean() ** 0.5
            st.write(f'Variable {i+1}, perturbation effect: {effect:.4f}')
    print('completed')
            
      



if __name__ == '__model_predictons__':
    model_predictons()
          

# k=0
# for j in range(0,10):
#  k=k+1
#  print(y_val_multi[j:k+1],multi_step_model.predict(x_val_multi[j:k+1]))

# for x, y in val_data_multi.take(10):
#  print(y_val_multi[:10],y[0])

# # dataframe=pd.DataFrame()
# for x, y in val_data_multi.take(10):
#   print(y[0],multi_step_model.predict(x)[0])
#   # dataset=(pd.DataFrame(y[0],multi_step_model.predict(x)[0]))
#   # dataframe=pd.concat([dataframe,dataset])

# dataframe['pred']=dataframe.index
# dataframe.reset_index(drop=True,inplace=True)
# dataframe.rename(columns={0:'actual'},inplace=True)

# dataframe['diff']=dataframe['actual']-dataframe['pred']

# dataframe.to_csv('v313_2_prediction.csv')

# df['Timestamp']=pd.to_datetime(df['Timestamp'],format = '%d-%m-%Y %H:%M')

# def func_daywise_spool1(data):
#     new_list = []
#     list_init = []
#     print_flag = False
#     for row, data in data.iterrows(): 
#         d = {}
#         d1 = {}
# #         list_timestamp.append(data['Spool-1_Length_Count_Actual'])
#         list_init = [data['Timestamp']]
# #         new_list = list_init + new_list
#         new_list = new_list + list_init
# #     print(new_list[0],new_list[-1])
#     d['Start_time'] = new_list[0]
#     d1['End_time'] =  new_list[-1]
#     strt_data = pd.DataFrame([d['Start_time'],d1['End_time']])
# #     end_data = pd.DataFrame(d1['End_time'])
    
#     return strt_data

# df['date']=df['Timestamp'].dt.date
# v314=df.groupby(['date']).apply(lambda x:func_daywise_spool1(x))

# solid_c12=v314[v314['level_1']==0]
# solid_c13=v314[v314['level_1']==1]
# solid_c12.reset_index(inplace=True)
# solid_c13.reset_index(inplace=True)
# deriming2=pd.concat([solid_c12,solid_c13],axis=1)
# deriming2=deriming2.drop(columns=['index','level_1'],axis=1)
# deriming2.drop(columns=['date'],inplace=True)
# deriming2.columns=['starttime','endtime']